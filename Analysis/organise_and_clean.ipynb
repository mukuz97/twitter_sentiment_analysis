{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoClient = MongoClient()\n",
    "db = mongoClient.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ConfigParser()\n",
    "parser.read('../config.ini')\n",
    "query_terms = list(parser.get('FILTER', 'filter_terms').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['text','timestamp','user','tweet_source','tweet_id','user_location','source_device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text(status):\n",
    "    if \"retweeted_status\" in status: # Check if Retweet\n",
    "        try:\n",
    "            return status[\"retweeted_status\"][\"extended_tweet\"][\"full_text\"]\n",
    "        except KeyError:\n",
    "            return status[\"retweeted_status\"][\"text\"]\n",
    "    else:\n",
    "        try:\n",
    "            return status[\"extended_tweet\"][\"full_text\"]\n",
    "        except KeyError:\n",
    "            return status[\"text\"]\n",
    "\n",
    "def get_tweet_source(status):\n",
    "    if \"retweeted_status\" in status:\n",
    "        return status[\"retweeted_status\"]['id_str']\n",
    "    else:\n",
    "        return status['id_str']\n",
    "\n",
    "def extract_source_device(html_data):\n",
    "    soup = BeautifulSoup(html_data, 'html.parser')\n",
    "    return soup.text\n",
    "\n",
    "def extract_tweet_data(status):\n",
    "    data = []\n",
    "    data.append(get_full_text(status))\n",
    "    data.append(int(status['timestamp_ms'])/1000)\n",
    "    data.append(status['user']['screen_name'])\n",
    "    data.append(get_tweet_source(status))\n",
    "    data.append(status['id_str'])\n",
    "    data.append(status['user']['location']) # User location is useless\n",
    "    data.append(extract_source_device(status['source']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Data CSV\n",
    "\n",
    "**Sampling method**\n",
    "- Taking 1% tweets of each company\n",
    "- Create a csv file of cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google samples: 4480\n",
      "google sample file created\n",
      "tesla samples: 777\n",
      "tesla sample file created\n",
      "apple samples: 4516\n",
      "apple sample file created\n",
      "spacex samples: 94\n",
      "spacex sample file created\n",
      "amazon samples: 3241\n",
      "amazon sample file created\n",
      "microsoft samples: 442\n",
      "microsoft sample file created\n",
      "facebook samples: 3395\n",
      "facebook sample file created\n"
     ]
    }
   ],
   "source": [
    "for term in query_terms:\n",
    "    count = db[term].estimated_document_count()\n",
    "    tweets = db[term].find()\n",
    "    sample_tweets = []\n",
    "    for index in np.random.choice(count, int(count / 100)):\n",
    "        sample_tweets.append(tweets[int(index)])\n",
    "    print(term, 'samples:', len(sample_tweets))\n",
    "\n",
    "    with open('../data/' + term + '_sample.csv', 'w', encoding='utf-8') as file:\n",
    "        csvwriter = csv.writer(file)\n",
    "        csvwriter.writerow(cols)\n",
    "        for tweet in sample_tweets:\n",
    "            csvwriter.writerow(extract_tweet_data(tweet))\n",
    "        print(term, 'sample file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Full Data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google file created\n",
      "tesla file created\n",
      "apple file created\n",
      "spacex file created\n",
      "amazon file created\n",
      "microsoft file created\n",
      "facebook file created\n"
     ]
    }
   ],
   "source": [
    "for term in query_terms:\n",
    "    with open('../data/' + term + '.csv', 'w', encoding='utf-8') as file:\n",
    "        csvwriter = csv.writer(file)\n",
    "        csvwriter.writerow(cols)\n",
    "        for tweet in db[term].find():\n",
    "            if tweet['lang'] == 'en':\n",
    "                csvwriter.writerow(extract_tweet_data(tweet))\n",
    "        print(term, 'file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "- Remove\n",
    "    - All urls\n",
    "    - '#' form infront of hashtags\n",
    "    - mentions\n",
    "    - emojis\n",
    "    - Non ASCII chanracters\n",
    "- Extract country form 'source_device'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import preprocessor as p\n",
    "from datetime import datetime\n",
    "from geotext import GeoText\n",
    "\n",
    "# Don't remove hastags\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.NUMBER, p.OPT.SMILEY, p.OPT.RESERVED, p.OPT.MENTION)\n",
    "\n",
    "NOT_BASIC_LATIN_PATTERN = re.compile(u'[^\\u0000-\\u007F]')\n",
    "PUNCTUATIONS_PATTERN = re.compile(r'[\\#\\$\\%\\&\\(\\)\\*\\+\\-\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]')\n",
    "MULTIPLE_SPACES_PATTERN = re.compile(r' +')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = p.clean(text) # Clean using tweet-preprocessor except hashtags\n",
    "    text = NOT_BASIC_LATIN_PATTERN.sub(' ', text) # Remove everything except basic latin\n",
    "    text = re.sub(r'&amp;', 'and', text)\n",
    "    text = PUNCTUATIONS_PATTERN.sub(' ', text) # Remove all punctuations\n",
    "    text = MULTIPLE_SPACES_PATTERN.sub(' ', text) # Remove multiple consequent spaces\n",
    "    return text.strip()\n",
    "\n",
    "def get_country(data):\n",
    "    country_men = list(GeoText(data).country_mentions.keys())\n",
    "    if len(country_men) > 0:\n",
    "        return country_men[0]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 31.2 ms, total: 2.83 s\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for term in query_terms:\n",
    "    file_addr = '../data/' + term + '_sample' + '.csv'\n",
    "    tweets = pd.read_csv(file_addr)\n",
    "    # Clean the tweet text\n",
    "    tweets['text'] = tweets['text'].apply(lambda x: clean_text(x))\n",
    "    # Extract 'country' from 'user_location'\n",
    "    tweets['country'] = tweets['user_location'].fillna('').apply(lambda x: get_country(x))\n",
    "    tweets = tweets.drop(['user_location'], axis=1)\n",
    "    tweets.to_csv(file_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 29s, sys: 4.28 s, total: 4min 34s\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for term in query_terms:\n",
    "    file_addr = '../data/' + term + '.csv'\n",
    "    tweets = pd.read_csv(file_addr)\n",
    "    # Clean the tweet text\n",
    "    tweets['text'] = tweets['text'].apply(lambda x: clean_text(x))\n",
    "    # Extract 'country' from 'user_location'\n",
    "    tweets['country'] = tweets['user_location'].fillna('').apply(lambda x: get_country(x))\n",
    "    tweets = tweets.drop(['user_location'], axis=1)\n",
    "    tweets.to_csv(file_addr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
